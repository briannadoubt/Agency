---
owner: null
agent_flow: null
agent_status: idle
branch: null
risk: normal
review: not-requested
---

# 12.2 OpenAI-Compatible Base Provider

Summary:
Implement a base provider for OpenAI-compatible APIs that can be used with Ollama, llama.cpp, vLLM, LM Studio, and other local model servers.

Acceptance Criteria:
- [ ] Create `OpenAICompatibleProvider` conforming to `AgentHTTPProvider`
- [ ] Implement chat completions request builder (`/v1/chat/completions`)
- [ ] Support streaming responses with SSE parsing
- [ ] Handle tool/function calling format (OpenAI tools schema)
- [ ] Parse streamed JSON delta responses
- [ ] Support configurable base URL and model name
- [ ] Optional API key header for authenticated endpoints
- [ ] Unit tests with mock HTTP responses

Notes:
- OpenAI format is the de facto standard for local model servers
- Stream format: `data: {"choices":[{"delta":{"content":"..."}}]}`
- Tool calls arrive as `{"choices":[{"delta":{"tool_calls":[...]}}]}`
- Consider URLSession with async bytes for streaming
- Handle `[DONE]` sentinel in SSE stream

History:
- 2025-12-18: Card created from Phase 12 planning.
