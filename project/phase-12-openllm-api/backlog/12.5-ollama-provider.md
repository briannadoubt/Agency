---
owner: null
agent_flow: null
agent_status: idle
branch: null
risk: low
review: not-requested
---

# 12.5 Ollama Provider Integration

Summary:
Implement a concrete Ollama provider that extends the OpenAI-compatible base with Ollama-specific features like model management and native API support.

Acceptance Criteria:
- [ ] Create `OllamaProvider` extending `OpenAICompatibleProvider`
- [ ] Default endpoint: `http://localhost:11434/v1/`
- [ ] Implement model listing via Ollama API (`/api/tags`)
- [ ] Add health check for Ollama server availability
- [ ] Support both OpenAI-compatible and native Ollama endpoints
- [ ] Handle Ollama-specific response format differences
- [ ] Add provider to registry with auto-discovery
- [ ] Settings UI for Ollama endpoint and model selection
- [ ] Integration tests with local Ollama (when available)

Notes:
- Ollama is the most popular local model server
- Supports both `/api/chat` (native) and `/v1/chat/completions` (OpenAI-compatible)
- Model names like "llama3.2", "codellama", "deepseek-coder"
- Consider auto-detecting Ollama on default port
- Tool use support varies by model (llama3.2 supports it)

History:
- 2025-12-18: Card created from Phase 12 planning.
