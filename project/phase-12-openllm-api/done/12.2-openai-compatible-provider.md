---
owner: claude
agent_flow: implement
agent_status: succeeded
branch: claude/complete-phase-12-1Ebkk
risk: normal
review: not-requested
---

# 12.2 OpenAI-Compatible Base Provider

Summary:
Implement a base provider for OpenAI-compatible APIs that can be used with Ollama, llama.cpp, vLLM, LM Studio, and other local model servers.

Acceptance Criteria:
- [x] Create `OpenAICompatibleProvider` conforming to `AgentHTTPProvider`
- [x] Implement chat completions request builder (`/v1/chat/completions`)
- [x] Support streaming responses with SSE parsing
- [x] Handle tool/function calling format (OpenAI tools schema)
- [x] Parse streamed JSON delta responses
- [x] Support configurable base URL and model name
- [x] Optional API key header for authenticated endpoints
- [x] Unit tests with mock HTTP responses

Notes:
- OpenAI format is the de facto standard for local model servers
- Stream format: `data: {"choices":[{"delta":{"content":"..."}}]}`
- Tool calls arrive as `{"choices":[{"delta":{"tool_calls":[...]}}]}`
- Consider URLSession with async bytes for streaming
- Handle `[DONE]` sentinel in SSE stream

History:
- 2025-12-18: Card created from Phase 12 planning.
- 2025-12-20: Implemented OpenAICompatibleProvider with full request building, SSE stream parsing, tool call support, and unit tests.
