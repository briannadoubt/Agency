---
owner: human
risk: normal
---

# Phase 12: OpenLLM API Support

## Goal

Extend Agency to support HTTP-based model providers beyond CLI tools. This enables users to run agent flows using OpenAI-compatible APIs (vLLM, llama.cpp server, Ollama, text-generation-webui), direct provider APIs (OpenAI, Google Gemini, Anthropic API), and self-hosted model servers.

## Motivation

The current architecture uses `AgentCLIProvider` to execute agent flows via CLI tools like Claude Code. However, many users want to:

1. Use local models via OpenAI-compatible servers (Ollama, llama.cpp, vLLM)
2. Connect directly to cloud APIs without CLI dependencies
3. Switch between providers based on task complexity or cost
4. Use specialized models for specific flows (e.g., fast local model for research, powerful cloud model for implementation)

## Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────┐
│                         ProviderRegistry                            │
├─────────────────────────────────────────────────────────────────────┤
│  AgentCLIProvider (existing)    │    AgentHTTPProvider (new)        │
│  ├── ClaudeCodeProvider         │    ├── OpenAICompatibleProvider   │
│  └── (future CLI providers)     │    ├── OllamaProvider             │
│                                 │    ├── OpenAIProvider             │
│                                 │    └── GeminiProvider             │
└─────────────────────────────────────────────────────────────────────┘
                                  │
                    ┌─────────────┴─────────────┐
                    │     GenericHTTPExecutor    │
                    │  (parallel to GenericCLI)  │
                    └───────────────────────────┘
```

### Key Components

- **AgentHTTPProvider**: New protocol for HTTP-based model providers
- **HTTPProviderRegistry**: Extension to ProviderRegistry for HTTP providers
- **GenericHTTPExecutor**: Executor that handles HTTP streaming responses
- **OpenAICompatibleProvider**: Base provider for OpenAI API format (covers most local servers)
- **Provider-specific implementations**: Ollama, OpenAI, Gemini, etc.

## OpenAI-Compatible API Standard

Most local model servers implement the OpenAI chat completions API:

```
POST /v1/chat/completions
{
  "model": "llama3.2",
  "messages": [{"role": "user", "content": "..."}],
  "stream": true,
  "tools": [...],
  "tool_choice": "auto"
}
```

This includes:
- Ollama (`http://localhost:11434/v1/`)
- llama.cpp server (`http://localhost:8080/v1/`)
- vLLM (`http://localhost:8000/v1/`)
- text-generation-webui (`http://localhost:5000/v1/`)
- LM Studio (`http://localhost:1234/v1/`)
- LocalAI (`http://localhost:8080/v1/`)

## Agent Loop Implementation

Unlike CLI providers where the agent loop is handled by the CLI tool, HTTP providers require implementing the agent loop in Agency:

```
┌─────────────────────────────────────────────────────────────────────┐
│                        AgentLoopController                          │
├─────────────────────────────────────────────────────────────────────┤
│  1. Build system prompt from card context                           │
│  2. Send messages to model via HTTP provider                        │
│  3. Parse response for tool calls                                   │
│  4. Execute tools (Read, Write, Bash, etc.)                         │
│  5. Append tool results to conversation                             │
│  6. Repeat until completion or max turns                            │
│  7. Update card with results                                        │
└─────────────────────────────────────────────────────────────────────┘
```

### Tool Execution Sandbox

HTTP-based agents need the same sandboxed tool execution as CLI agents:
- Reuse existing XPC worker infrastructure for tool execution
- Worker receives tool call requests, executes safely, returns results
- Maintains security model from Phase 5/9

## Tasks

- 12.1 - HTTP Provider Protocol & Registry
- 12.2 - OpenAI-Compatible Base Provider
- 12.3 - Agent Loop Controller
- 12.4 - Tool Execution Bridge
- 12.5 - Ollama Provider Integration
- 12.6 - Provider Settings UI
- 12.7 - Testing & Documentation

## Dependencies

- Phase 5 (Agent Integration) - XPC worker infrastructure
- Phase 9 (Claude Code) - CLI provider patterns, streaming output
- Existing PromptBuilder system for context construction

## Security Considerations

- API keys stored in Keychain (reuse ClaudeKeyManager pattern)
- Local servers may not require authentication
- Tool execution sandboxed via XPC workers
- Network access limited to configured endpoints

## Success Criteria

- [x] Users can run agent flows using Ollama with local models
- [x] Users can connect to OpenAI API directly (no CLI required)
- [x] Provider switching available in card run UI
- [x] Streaming output works for HTTP providers
- [x] Cost tracking for metered APIs
- [x] Settings UI for configuring provider endpoints and API keys

History:
- 2025-12-18 - Phase created for OpenLLM API support
- 2025-12-20 - Phase completed with full implementation of HTTP provider system
